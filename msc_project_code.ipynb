{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "msc_project_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_RU-A1Er96l"
      },
      "source": [
        "# Expectation-Maximisation algorithm with Maximum Likelihood part"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA_AgIHSk-VL"
      },
      "source": [
        "class EM_ML:\n",
        "    # number of actions(num_actions), number of states(num_states), discount parameter(gamma), number of runs in each episode (num_in_episode)\n",
        "    def __init__(self, num_actions, num_states, gamma, num_in_episode):\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.num_states = num_states\n",
        "        self.gamma = gamma\n",
        "        self.num_in_episode = num_in_episode\n",
        "        self.reward = np.zeros((num_states, num_actions)) # r(s,a), which is represented by the (num_states, num_actions) matrix\n",
        "        self.policy = np.random.random((num_actions, num_states)) # p(a|s), which is represented by (num_actions, num_states) matrix\n",
        "        self.policy /= np.sum(self.policy, axis=0)  # normalisation, i.e. sum to 1 for each column\n",
        "        self.message_left = np.zeros((num_in_episode, num_states)) # message passing from 1 to time t, where t is number of runs in each episode\n",
        "        self.message_right = np.zeros((num_in_episode, num_states, num_actions)) # message passing from t to time 1\n",
        "        self.alpha = np.ones((num_states, num_states, num_actions)) # prior for transition parameter theta, where all alphas are set to 1\n",
        "        self.theta = self.alpha / np.sum(self.alpha, axis=0)  # normalisation, i.e. sum to 1 for given state and action\n",
        "        self.start_dist = np.ones(num_states) # start distribution\n",
        "        self.start_dist /= np.sum(self.start_dist) # normalisation, i.e. sum to 1 for all states\n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "\n",
        "\n",
        "    # Update the transition parameter\n",
        "    def update_theta(self):\n",
        "\n",
        "        self.theta = self.alpha / np.sum(self.alpha, axis=0)  # normalisation, i.e. sum to 1 for given state and action\n",
        "\n",
        "\n",
        "    # E-step \n",
        "    def Estep(self):\n",
        "\n",
        "        for i in range(self.num_in_episode):\n",
        "          # calculate left(alpha) message and right(beta) message\n",
        "            if i == 0:\n",
        "                self.message_right[i, :, :] = self.reward\n",
        "                self.message_left[i, :] = self.start_dist\n",
        "            else:\n",
        "                self.message_right[i, :, :] = np.einsum('ij,ixy,ji->xy', self.message_right[i-1, :, :], self.theta, self.policy)\n",
        "                self.message_left[i, :] = np.einsum('xij,ji,i->x', self.theta, self.policy, self.message_left[i-1, :])\n",
        "\n",
        "    # M-step\n",
        "    def Mstep(self):\n",
        "        previous_policy = np.copy(self.policy)\n",
        "        discounted_message_prod = np.zeros((self.num_actions, self.num_states))\n",
        "        for tt in range(1, self.num_in_episode+1):\n",
        "            discounted_message_prod += (self.gamma**(tt-1) * np.einsum('txa,tx->ax', np.flip(self.message_right[:tt, :, :], axis=0), self.message_left[:tt, :]))\n",
        "        self.policy = self.policy* discounted_message_prod # update policy\n",
        "        self.policy = self.policy + 1e-100  # give a small value for every useless actions under some states\n",
        "        self.policy /= np.sum(self.policy, axis=0)  # normalisation\n",
        "        differnece_policy = np.linalg.norm(previous_policy - self.policy, ord=1) # difference between old policy and updated policy\n",
        "        return differnece_policy \n",
        "\n",
        "\n",
        "    # keep same starting situation for each episode\n",
        "    def back(self):\n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "\n",
        "    # iterate until satisfies convergence criteria\n",
        "    def iteration(self, num_Iter=None, tol=0.01):\n",
        "      self.update_theta()\n",
        "      if num_Iter is None:  \n",
        "        norm_diff = np.inf\n",
        "        visualisation = tqdm()\n",
        "        while norm_diff > tol: #loop until satisfies tolerance, i.e. convergence criteria\n",
        "          self.Estep()\n",
        "          norm_diff = self.Mstep()\n",
        "          visualisation.update(1)\n",
        "        visualisation.close()\n",
        "      else:  # update for given number of steps\n",
        "        for _ in tqdm(range(int(num_Iter))):\n",
        "          self.Estep()\n",
        "          self.Mstep()\n",
        "\n",
        "\n",
        "    # choose action under policy distribution\n",
        "    def make_action(self, state):\n",
        "        action = np.random.choice(self.num_actions, p=self.policy[:, state]) # select action under current policy distribution\n",
        "        if (self.last_state is not None) and (self.last_action is not None):\n",
        "            self.alpha[state, self.last_state, self.last_action] += 1 # update count with observation \n",
        "\n",
        "        self.last_state = state\n",
        "        self.last_action = action\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "    def tool(self):\n",
        "      return self.policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P89ke0VJcxZ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sptPBmGkcxjK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRp7Bzv1H2So"
      },
      "source": [
        "# Expectation-Maximisation algorithm with Variational Bayes part"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hADdVC3ycAWB"
      },
      "source": [
        "class EM_VB:\n",
        "\n",
        "    # number of actions(num_actions), number of states(num_states), discount parameter(gamma), number of runs in each episode (num_in_episode)\n",
        "    def __init__(self, num_actions, num_states, gamma, num_in_episode):\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.num_states = num_states\n",
        "        self.gamma = gamma\n",
        "        self.num_in_episode = num_in_episode\n",
        "        self.reward = np.zeros((num_states, num_actions)) # r(s,a), which is represented by the (num_states, num_actions) matrix\n",
        "        self.policy = np.random.random((num_actions, num_states)) # p(a|s), which is represented by (num_actions, num_states) matrix\n",
        "        self.policy /= np.sum(self.policy, axis=0)  # normalisation, i.e. sum to 1 for each column\n",
        "        self.message_left = np.zeros((num_in_episode, num_states)) # message passing from 1 to time t, where t is number of runs in each episode (alpha_{\\tau})\n",
        "        self.message_right = np.zeros((num_in_episode, num_states, num_actions)) # message passing from t to time 1 (beta_{t-\\tau})\n",
        "        self.alpha = np.ones((num_states, num_states, num_actions)) # prior for transition parameter theta, where all alphas are set to 1\n",
        "        self.theta = self.alpha / np.sum(self.alpha, axis=0)  # normalisation, i.e. sum to 1 for given state and action\n",
        "        self.start_dist = np.ones(num_states) # start distribution\n",
        "        self.start_dist /= np.sum(self.start_dist) # normalisation, i.e. sum to 1 for all states\n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "\n",
        "    # E-step under Variational Bayes\n",
        "    def Estep(self):\n",
        "\n",
        "        local_message_left = np.zeros((self.num_in_episode-1, self.num_states)) # local message from 1 to time t-1, where t is number of runs in each episode (a_{\\tau})\n",
        "        local_message_right = np.zeros((self.num_in_episode-1, self.num_states))  # local message from t-1 to time 1, where t is number of runs in each episode (b_{t-\\tau-1})\n",
        "        tilde_r = np.zeros((self.num_states, self.num_states, self.num_actions)) #\\tilde{r}, which is tensor of (num_states, num_states, num_actions)\n",
        "        for _ in range(10): \n",
        "            q_distribution = np.zeros((self.num_states, self.num_states, self.num_actions, self.num_in_episode-1, self.num_in_episode-1)) # initialise q distribution, q(s_{\\tau+1}, s_{\\tau}, a_{\\tau}, t)\n",
        "            log_q_theta =  digamma(self.alpha+tilde_r) - digamma(np.sum(self.alpha+tilde_r, axis=0)) # using standard digamma to calculate expectation of log_\\theta under distribution q_\\theta\n",
        "            exp_log_q_theta =  np.exp(log_q_theta)\n",
        "            \n",
        "            for i in range(self.num_in_episode-1): \n",
        "                if i == 0:\n",
        "                    local_message_right[i, :] = np.einsum('as,sa->s', self.policy,self.reward)\n",
        "                    local_message_left[i, :] = self.start_dist\n",
        "                else:\n",
        "                    local_message_right[i, :] = np.einsum('i,ijk,kj->j',local_message_right[i-1, :], exp_log_q_theta, self.policy)\n",
        "                    local_message_left[i, :] = np.einsum('xij,ji,i->x', exp_log_q_theta, self.policy, local_message_left[i-1, :])\n",
        "\n",
        "            for tt in range(1, self.num_in_episode):\n",
        "              # update q distribution by using local message \n",
        "                q_distribution[:, :, :, 0:tt, tt-1] = self.gamma**(tt) * np.einsum('tp,ap,npa,tn->npat', local_message_left[:tt, :], self.policy, exp_log_q_theta, np.flip(local_message_right[:tt, :], axis=0))\n",
        "            q_distribution /= np.sum(q_distribution) # normalisation, i.e. sum to 1\n",
        "\n",
        "            tilde_r = np.einsum('npaut->npa', q_distribution) # calculation of \\tilde_r\n",
        "\n",
        "        for i in range(self.num_in_episode):\n",
        "          # update global message\n",
        "            if i == 0:\n",
        "                self.message_right[i, :, :] = self.reward\n",
        "                self.message_left[i, :] = self.start_dist\n",
        "            else:\n",
        "                self.message_right[i, :, :] = np.einsum('ij,ixy,ji->xy', self.message_right[i-1, :, :], exp_log_q_theta, self.policy)\n",
        "                self.message_left[i, :] = np.einsum('xij,ji,i->x', exp_log_q_theta, self.policy, self.message_left[i-1, :])\n",
        "\n",
        "\n",
        "    # M-step, its formula is same as normal EM\n",
        "    def Mstep(self):\n",
        "        previous_policy = np.copy(self.policy)\n",
        "        discounted_message_prod = np.zeros((self.num_actions, self.num_states))\n",
        "        for tt in range(1, self.num_in_episode+1):\n",
        "            discounted_message_prod += (self.gamma**(tt-1) * np.einsum('txa,tx->ax', np.flip(self.message_right[:tt, :, :], axis=0), self.message_left[:tt, :]))\n",
        "        self.policy = self.policy* discounted_message_prod # update policy\n",
        "        self.policy /= np.sum(self.policy, axis=0)  # normalisation\n",
        "        differnece_policy = np.linalg.norm(previous_policy - self.policy, ord=1) # difference between old policy and updated policy\n",
        "        return differnece_policy \n",
        "\n",
        "    # keep same starting situation for each episode\n",
        "    def back(self):\n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "\n",
        "\n",
        "    # iterate until satisfies convergence criteria\n",
        "    def iteration(self, num_Iter=None, tol=0.01):\n",
        "      if num_Iter is None:  \n",
        "        norm_diff = np.inf\n",
        "        visualisation = tqdm()\n",
        "        while norm_diff > tol: #loop until satisfies tolerance, i.e. convergence criteria\n",
        "          self.Estep()\n",
        "          norm_diff = self.Mstep()\n",
        "          visualisation.update(1)\n",
        "        visualisation.close()\n",
        "      else:  # update for given number of steps\n",
        "        for _ in tqdm(range(int(num_Iter))):\n",
        "          self.Estep()\n",
        "          self.Mstep()\n",
        "\n",
        "\n",
        "    # choose action under policy distribution\n",
        "    def make_action(self, state):\n",
        "        action = np.random.choice(self.num_actions, p=self.policy[:, state]) # select action under current policy distribution\n",
        "        if (self.last_state is not None) and (self.last_action is not None):\n",
        "            self.alpha[state, self.last_state, self.last_action] += 1 # update count with observation \n",
        "\n",
        "        self.last_state = state\n",
        "        self.last_action = action\n",
        "\n",
        "        return action\n",
        "\n",
        "    def tool(self):\n",
        "      return self.policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwGzOycFcykH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6u74w5pcyqh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpweE6CeudR9"
      },
      "source": [
        "# Dyna-Q algorithm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ULZ-EAuwQMY"
      },
      "source": [
        "# table model for the environment\n",
        "class TabularModel(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions):\n",
        "    self.table_for_next_state = np.zeros((number_of_states,number_of_actions))\n",
        "    self.table_for_reward = np.zeros((number_of_states,number_of_actions))\n",
        "    self.table_for_discount = np.zeros((number_of_states,number_of_actions))\n",
        "\n",
        "  def next_state(self, s, a):\n",
        "    return self.table_for_next_state[s,a]\n",
        "  \n",
        "  def reward(self, s, a):\n",
        "    return self.table_for_reward[s,a]\n",
        "\n",
        "  def discount(self, s, a):\n",
        "    return self.table_for_discount[s,a]\n",
        "  \n",
        "  def transition(self, state, action):\n",
        "    return (\n",
        "        self.reward(state, action), \n",
        "        self.discount(state, action),\n",
        "        self.next_state(state, action))\n",
        "  \n",
        "  def update(self, state, action, reward, discount, next_state):\n",
        "    self.table_for_next_state[state,action] = next_state\n",
        "    self.table_for_reward[state,action] = reward\n",
        "    self.table_for_discount[state,action] = discount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp_Ls3SyfhGp"
      },
      "source": [
        "# epsilon-greedy\n",
        "def epsilon_greedy(q_values, state):\n",
        "  if 0.3 < np.random.random():\n",
        "    maxq = np.max(q_values[state,:])\n",
        "    matches = [i for i in range(q_values.shape[1]) if q_values[state, i] == maxq]\n",
        "    return np.random.choice(matches)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN_z83nVCG8z"
      },
      "source": [
        "class DynaQ(object):\n",
        "\n",
        "  def __init__(\n",
        "      self,  number_of_actions, number_of_states, initial_state, \n",
        "      behaviour_policy, Q, num_offline_updates=0, step_size=0.1):\n",
        "    \n",
        "    self._state = initial_state\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._num_offline_updates = num_offline_updates\n",
        "    self._lr = step_size\n",
        "    self._Q = Q\n",
        "    self._action = self._behaviour_policy(self._Q, self._state)\n",
        "    self._model = TabularModel(number_of_states, number_of_actions)\n",
        "    self._replay_buffer = []  \n",
        "    \n",
        "  # action-value function\n",
        "  def q_values(self):\n",
        "    return self._Q\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "\n",
        "    self._Q[s,a] += self._lr * (r + g*np.max(self._Q[next_s,:]) - self._Q[s,a])\n",
        "    self._replay_buffer.append((s,a))\n",
        "    self._model.update(s, a, r, g, next_s)\n",
        "    # offline updates\n",
        "    for n in range(self._num_offline_updates):\n",
        "      index = np.random.randint(len(self._replay_buffer))\n",
        "      s, a = self._replay_buffer[index]\n",
        "      r, g, next_s = self._model.transition(s, a)\n",
        "      self._Q[s,a] += self._lr * (r + g*np.max(self._Q[int(next_s),:]) - self._Q[s,a])\n",
        "\n",
        "    next_action = self._behaviour_policy(self._Q, self._state)\n",
        "    self._state = next_state\n",
        "    self._action = next_action\n",
        "\n",
        "    return self._action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3KoQXkEssJv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TucrnZ2YunSS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MRb4y5DunYi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTTmDVnmungT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjtEXaPYwWI6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOFArbdXCG_H"
      },
      "source": [
        "!pip install pycolab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad8i0Bp2CHEU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import basic\n",
        "import problem\n",
        "import random\n",
        "import grid_world\n",
        "from scipy.special import digamma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb00LepQdMBv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpz5Lb1SdMKX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKyAy3CGiuLS"
      },
      "source": [
        "# chain problem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS7gXwgbCHLw"
      },
      "source": [
        "# Environment settings for chain problem\n",
        "gamma = 0.8\n",
        "num_actions = 2\n",
        "num_states = 5\n",
        "reward_table = np.array([[0, 2],[0, 2],[0, 2],[0, 2], [10, 2]])\n",
        "num_in_episode = 100\n",
        "num_episodes = 20\n",
        "num_experiments = 50\n",
        "start_dist = np.array([1, 0, 0, 0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izBcc84PpmZ2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPc1aivzpmcr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAQIHt5tui8Q"
      },
      "source": [
        "reward_hist_each_experiment_EM_ML = []\n",
        "learned_policy_EM_ML = []\n",
        "for _ in tqdm(range(num_experiments)):\n",
        "    emml = EM_ML(num_actions, num_states, gamma, num_in_episode)\n",
        "    emml.reward = np.copy(reward_table)\n",
        "    emml.update_theta()\n",
        "    emml.start_dist = np.copy(start_dist)\n",
        "    reward_hist_each_episode_EM_ML = []\n",
        "    for _ in range(num_episodes):\n",
        "        \n",
        "        culmulative_reward = 0\n",
        "        emml.back()\n",
        "        case = problem.chain()\n",
        "        observation = case.its_showtime()\n",
        "        for _ in range(num_in_episode):\n",
        "            action = emml.make_action(basic.location(observation, char='O'))\n",
        "            observation = case.play(action)\n",
        "            if not observation[1] is None:\n",
        "                culmulative_reward += observation[1]\n",
        "        #learn after the end of episode\n",
        "        emml.iteration(tol=0.01)\n",
        "        # add average reward to list\n",
        "        reward_hist_each_episode_EM_ML.append(culmulative_reward/num_in_episode)\n",
        "        # quit game\n",
        "        case.play(5)\n",
        "    learned_policy_EM_ML.append(emml.tool())\n",
        "    reward_hist_each_experiment_EM_ML.append(reward_hist_each_episode_EM_ML)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6-KIMS5pm4x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZYRcHgpm6p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf_TAI8QCHNp"
      },
      "source": [
        "reward_hist_each_experiment_EM_VB = []\n",
        "learned_policy_EM_VB = []\n",
        "for _ in tqdm(range(num_experiments)):\n",
        "    emvb = EM_VB(num_actions, num_states, gamma, num_in_episode)\n",
        "    emvb.reward = np.copy(reward_table)\n",
        "    emvb.start_dist = np.copy(start_dist)\n",
        "    reward_hist_each_episode_EM_VB = []\n",
        "    for _ in range(num_episodes):\n",
        "        \n",
        "        culmulative_reward = 0\n",
        "        emvb.back()\n",
        "        case = problem.chain()\n",
        "        observation = case.its_showtime()\n",
        "        for _ in range(num_in_episode):\n",
        "            action = emvb.make_action(basic.location(observation, char='O'))\n",
        "            observation = case.play(action)\n",
        "            if not observation[1] is None:\n",
        "                culmulative_reward += observation[1]\n",
        "        #learn after the end of episode\n",
        "        emvb.iteration(tol=0.01)\n",
        "        # add average reward to list\n",
        "        reward_hist_each_episode_EM_VB.append(culmulative_reward/num_in_episode)\n",
        "        # quit game\n",
        "        case.play(5)\n",
        "    learned_policy_EM_VB.append(emvb.tool())\n",
        "    reward_hist_each_experiment_EM_VB.append(reward_hist_each_episode_EM_VB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAWl7dyDpnfs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTpe_gSGpnh8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6is42-Ppol4P"
      },
      "source": [
        "# Notice the combination of num_offline, epsilon-greedy, step_size, discount\n",
        "reward_hist_each_experiment_DynaQ = []\n",
        "learned_Q = []\n",
        "for _ in tqdm(range(50)):\n",
        "  Q = np.random.random((5,2))\n",
        "  agent = DynaQ(2, 5, 0, epsilon_greedy, Q, num_offline_updates=30, step_size=0.1)\n",
        "  mean_reward = 0.\n",
        "  reward_hist_each_episode_DynaQ = []\n",
        "  #reward_hist_each_episode_DynaQ.append(mean_reward)\n",
        "  case = problem.chain()\n",
        "  observation = case.its_showtime()\n",
        "  observation = case.play(0)\n",
        "  for i in range(100):\n",
        "    r = observation[1]\n",
        "    if observation[1] == None:\n",
        "      r = 0\n",
        "    ns = basic.location(observation, char='O')\n",
        "    action = agent.step(reward = r, discount = 0.8, next_state = ns)\n",
        "    observation = case.play(action)\n",
        "    mean_reward += (r - mean_reward)/(i + 1.)\n",
        "    print(r)\n",
        "  reward_hist_each_episode_DynaQ.append(mean_reward)\n",
        "  for __ in range(19):\n",
        "    agent = DynaQ(2, 5, 0, epsilon_greedy, Q, num_offline_updates=30, step_size=0.1)\n",
        "    case = problem.chain()\n",
        "    observation = case.its_showtime()\n",
        "    observation = case.play(0)\n",
        "    mean_reward = 0.\n",
        "    for i in range(100):\n",
        "      r = observation[1]\n",
        "      if observation[1] == None:\n",
        "        r = 0\n",
        "      ns = basic.location(observation, char='O')\n",
        "      action = agent.step(reward = r, discount = 0.8, next_state = ns)\n",
        "      observation = case.play(action)\n",
        "      mean_reward += (r - mean_reward)/(i + 1.)\n",
        "    #print(r)\n",
        "    reward_hist_each_episode_DynaQ.append(mean_reward)\n",
        "  learned_Q.append(Q)\n",
        "  reward_hist_each_experiment_DynaQ.append(reward_hist_each_episode_DynaQ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPl9LZ9kfuP1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUdzLgQIfRuc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcIcr3t0ol9d"
      },
      "source": [
        "#plot for experiment results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Pzq5L4mGpC"
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "red_patch = mpatches.Patch(color='red', label='EM-ML')\n",
        "black_patch = mpatches.Patch(color='black', label='EM-VB')\n",
        "blue_patch = mpatches.Patch(color='blue', label='Dyna-Q')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlYOVaPYglzM"
      },
      "source": [
        "emml_reward_data = np.array(reward_hist_each_experiment_EM_ML)\n",
        "emvb_reward_data = np.array(reward_hist_each_experiment_EM_VB)\n",
        "DynaQ_reward_data = np.array(reward_hist_each_experiment_DynaQ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwsKphoqEDzF"
      },
      "source": [
        "log = pd.DataFrame(columns=['episodes','average reward','Algorithm'])\n",
        "for nx in range(50):\n",
        "    for ne in range(20):\n",
        "        log=log.append(pd.DataFrame({'episodes': ne+1,\n",
        "                                    'average reward': emml_reward_data[nx, ne],\n",
        "                                    'Algorithm': \"EM-ML\"}, index=[log.size+1]))\n",
        "log1 = pd.DataFrame(columns=['episodes','average reward','Algorithm'])\n",
        "for nx in range(50):\n",
        "    for ne in range(20):\n",
        "        log1=log1.append(pd.DataFrame({'episodes': ne+1,\n",
        "                                    'average reward': emvb_reward_data[nx, ne],\n",
        "                                    'Algorithm': \"EM-VB\"}, index=[log1.size+1]))\n",
        "log2 = pd.DataFrame(columns=['episodes','average reward','Algorithm'])\n",
        "for nx in range(50):\n",
        "    for ne in range(20):\n",
        "        log2=log2.append(pd.DataFrame({'episodes': ne+1,\n",
        "                                    'average reward': DynaQ_reward_data[nx, ne],\n",
        "                                    'Algorithm': \"Dyna-Q\"}, index=[log2.size+1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKHZJ4nOGs2w"
      },
      "source": [
        "sns.set_style('white')\n",
        "A = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log, color=\"red\", label = \"EM-ML\", ci = 90)\n",
        "B = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log1, color=\"black\", label = \"EM-VB\", ci = 90)\n",
        "C = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log2, color= \"blue\", label = \"Dyna-Q\", ci = 90)\n",
        "plt.legend(handles=[red_patch, black_patch, blue_patch])\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('average reward')\n",
        "plt.ylim(1,3.4)\n",
        "plt.title('EM-ML, EM-VB and Dyna-Q on chain problem with confidence interval')\n",
        "plt.savefig('ENV1_with_ci')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7xAWMWWHsJ2"
      },
      "source": [
        "A = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log, color=\"red\", label = \"EM-ML\", ci = None)\n",
        "B = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log1, color=\"black\", label = \"EM-VB\", ci= None)\n",
        "C = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log2, color= \"blue\", label = \"Dyna-Q\", ci = None)\n",
        "plt.legend(handles=[red_patch, black_patch, blue_patch])\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('average reward')\n",
        "plt.ylim(1,3.4)\n",
        "plt.title('EM-ML, EM-VB and Dyna-Q on chain problem')\n",
        "plt.savefig('ENV1_without_ci')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RxoEw62uYv8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPgo97ckiHtR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdimpmollmVg"
      },
      "source": [
        "# increase the number of episodes, then run the reward_hist_each_experiment_DynaQ result with this code\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(9, 6), dpi=100)\n",
        "DynaQ_reward_data = np.array(reward_hist_each_experiment_DynaQ)\n",
        "C = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log3, color= \"blue\",label = \"Dyna-Q\",ci = 90)\n",
        "plt.title('Dyna-Q with larger number of episodes on chain problem')\n",
        "plt.savefig('Dyna_longer')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVuGZ7C_9gxn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmP9wCQ5vaEJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIKC_A49vaM9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihY1w0CC9C4d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6G0tgBkq_Ce"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud56k5bkq_JO"
      },
      "source": [
        "# Grid world problem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-0Xidobuols"
      },
      "source": [
        "import grid_world"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbbp5S6RQ5ui"
      },
      "source": [
        "# experiment setting for the grid world problem\n",
        "grid = grid_world.make_game()\n",
        "result = grid.its_showtime()\n",
        "num_experiments = 30\n",
        "grid_param = grid_world.get_params()\n",
        "num_states = grid_param['num_states']\n",
        "num_actions = grid_param['num_actions']\n",
        "num_in_episode = 100\n",
        "gamma = 0.9\n",
        "num_episodes = 20\n",
        "\n",
        "# Notice that the reward as 0 for moving to non-goal state and the reward as 50 for moving to goal state\n",
        "reward = np.zeros((num_states, num_actions))\n",
        "goal_loc = basic.location(result, 'X')\n",
        "reward[goal_loc+1, 0] = 50\n",
        "reward[goal_loc-1, 1] = 50\n",
        "reward[goal_loc+11, 2] = 50\n",
        "reward[goal_loc-11, 3] = 50\n",
        "start_loc = basic.location(result, 'O')\n",
        "start_dist = np.zeros(num_states)\n",
        "start_dist[start_loc] = 1\n",
        "\n",
        "# transition count\n",
        "alpha = np.zeros((num_states, num_states, num_actions))+0.1\n",
        "for state in range(num_states):\n",
        "    # move to next state if it is not a wall\n",
        "    if not basic.is_wall(state, result):\n",
        "        if not basic.is_wall(state-1, result):\n",
        "            alpha[state-1, state, 0] = 1\n",
        "        else:\n",
        "            alpha[state, state, 0] = 1\n",
        "        if not basic.is_wall(state+1, result):\n",
        "            alpha[state+1, state, 1] = 1\n",
        "        else:  \n",
        "            alpha[state, state, 1] = 1\n",
        "        if not basic.is_wall(state-11, result):\n",
        "            alpha[state-11, state, 2] = 1\n",
        "        else:  \n",
        "            alpha[state, state, 2] = 1\n",
        "        if not basic.is_wall(state+11, result):\n",
        "            alpha[state+11, state, 3] = 1\n",
        "        else:\n",
        "            alpha[state, state, 3] = 1\n",
        "    # if current state is wall,  stay inside wall no matter which action\n",
        "    else: \n",
        "        alpha[state, state, :] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVDWWlnfprPF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gODOnCjprRp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBhgEiGtoMLY"
      },
      "source": [
        "# initialize policy to sub-optimal\n",
        "emml_policy = np.ones((num_actions, num_states))\n",
        "emml_policy[3, 24] = 10\n",
        "emml_policy[3, 35] = 10\n",
        "emml_policy[3, 46] = 10\n",
        "emml_policy[1, 57] = 10\n",
        "emml_policy[1, 58] = 10\n",
        "emml_policy[1, 59] = 10\n",
        "emml_policy[1, 60] = 10\n",
        "emml_policy[1, 61] = 10\n",
        "emml_policy[1, 62] = 10\n",
        "emml_policy[2, 63] = 10\n",
        "emml_policy[2, 52] = 10\n",
        "emml_policy[2, 41] = 10\n",
        "emml_policy[2, 30] = 10\n",
        "emml_policy[2, 19] = 10\n",
        "emml_policy /= np.sum(emml_policy, axis=0)  # normalise to make distribution\n",
        "\n",
        "reward_hist_each_experiment_EM_ML = []\n",
        "learned_policy_EM_ML = []\n",
        "for _ in tqdm(range(num_experiments)):\n",
        "    emml = EM_ML(num_actions, num_states,gamma,num_in_episode)\n",
        "    emml.alpha = np.copy(alpha)\n",
        "    emml.update_theta()\n",
        "    emml.reward = np.copy(reward)\n",
        "    emml.start_dist = np.copy(start_dist)\n",
        "    emml.policy = np.copy(emml_policy)\n",
        "    reward_hist_each_episodes_EM_ML = []\n",
        "    for _ in range(num_episodes):\n",
        "        cumulative_reward = 0\n",
        "        emml.back()\n",
        "        game = grid_world.make_game()\n",
        "        obs = game.its_showtime()\n",
        "        for _ in range(num_in_episode):\n",
        "            action = emml.make_action(basic.location(obs, char='O'))\n",
        "            obs = game.play(action)\n",
        "            if not obs[1] is None:\n",
        "                cumulative_reward += obs[1]  \n",
        "        emml.iteration(tol=0.01)\n",
        "        reward_hist_each_episodes_EM_ML.append(cumulative_reward/num_in_episode)\n",
        "        game.play(5)\n",
        "        print(cumulative_reward/num_in_episode)\n",
        "    learned_policy_EM_ML.append(emml.tool())\n",
        "    reward_hist_each_experiment_EM_ML.append(reward_hist_each_episodes_EM_ML)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0NFh4J7YJeY"
      },
      "source": [
        "# Learned policy \n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,24]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,23]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,12]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,25]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,35]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,36]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,13]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,14]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,15]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,16]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,17]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,28]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,29]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,18]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,19]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,30]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,31]\n",
        "np.mean(np.array(learned_policy_EM_ML),axis =0)[:,41]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IibaLJ27J_DT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JoYNfSom7yB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HYqOreDMJsDP"
      },
      "source": [
        "# initialize policy to local minima\n",
        "emvb_policy = np.ones((num_actions, num_states))\n",
        "emvb_policy[3, 24] = 10\n",
        "emvb_policy[3, 35] = 10\n",
        "emvb_policy[3, 46] = 10\n",
        "emvb_policy[1, 57] = 10\n",
        "emvb_policy[1, 58] = 10\n",
        "emvb_policy[1, 59] = 10\n",
        "emvb_policy[1, 60] = 10\n",
        "emvb_policy[1, 61] = 10\n",
        "emvb_policy[1, 62] = 10\n",
        "emvb_policy[2, 63] = 10\n",
        "emvb_policy[2, 52] = 10\n",
        "emvb_policy[2, 41] = 10\n",
        "emvb_policy[2, 30] = 10\n",
        "emvb_policy[2, 19] = 10\n",
        "emvb_policy /= np.sum(emvb_policy, axis=0)  # normalize to make dist.\n",
        "reward_hist_each_experiment_EM_VB = []\n",
        "learned_policy_EM_VB = []\n",
        "for _ in tqdm(range(num_experiments)):\n",
        "    emvb = EM_VB(num_actions, num_states,gamma,num_in_episode)\n",
        "    emvb.alpha = np.copy(alpha)\n",
        "    emvb.reward = np.copy(reward)\n",
        "    emvb.start_dist = np.copy(start_dist)\n",
        "    emvb.policy = np.copy(emvb_policy)\n",
        "    reward_hist_each_episodes_EM_VB = []\n",
        "    for _ in range(num_episodes):\n",
        "        cumulative_reward = 0\n",
        "        emvb.back()\n",
        "        game = grid_world.make_game()\n",
        "        obs = game.its_showtime()\n",
        "        for _ in range(num_in_episode):\n",
        "            action = emvb.make_action(basic.location(obs, char='O'))\n",
        "            obs = game.play(action)\n",
        "            if not obs[1] is None:\n",
        "                cumulative_reward += obs[1]  \n",
        "        emvb.iteration(tol=0.01)\n",
        "        reward_hist_each_episodes_EM_VB.append(cumulative_reward/num_in_episode)\n",
        "        game.play(5)\n",
        "        print(cumulative_reward/num_in_episode)\n",
        "    learned_policy_EM_VB.append(vbem.tool())\n",
        "    reward_hist_each_experiment_EM_VB.append(reward_hist_each_episodes_EM_VB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWB9b0iYc5Lx"
      },
      "source": [
        "# Learned policy \n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,24]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,23]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,12]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,25]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,35]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,36]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,13]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,14]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,15]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,16]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,17]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,28]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,29]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,18]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,19]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,30]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,31]\n",
        "np.mean(np.array(learned_policy_EM_VB),axis =0)[:,41]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb56iARWc5Od"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcFF4rIGm3cg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrpaRsMvLfEs"
      },
      "source": [
        "reward_hist_each_experiment_DynaQ = []\n",
        "learned_Q = []\n",
        "for _ in tqdm(range(30)):\n",
        "  Q = np.zeros((99,4))+0.1\n",
        "  Q[24,3] = 10\n",
        "  Q[35,3] = 10\n",
        "  Q[46,3] = 10\n",
        "  Q[57,1] = 10\n",
        "  Q[58,1] = 10\n",
        "  Q[59,1] = 10\n",
        "  Q[60,1] = 10\n",
        "  Q[61,1] = 10\n",
        "  Q[62,1] = 10\n",
        "  Q[63,1] = 10\n",
        "  Q[52,2] = 10\n",
        "  Q[41,2] = 10\n",
        "  Q[30,2] = 10\n",
        "  Q[19,2] = 10\n",
        "  agent = DynaQ(4, 99, 24, epsilon_greedy, Q, num_offline_updates=100, step_size=0.2)\n",
        "  mean_reward = 0.\n",
        "  reward_hist_each_episode_DynaQ = []\n",
        "  #reward_hist_each_episode_DynaQ.append(mean_reward)\n",
        "  case = grid_world.make_game()\n",
        "  observation = case.its_showtime()\n",
        "  observation = case.play(0)\n",
        "  for i in range(100):\n",
        "    r = observation[1]\n",
        "    if observation[1] == None:\n",
        "      r = 0\n",
        "    ns = basic.location(observation, char='O')\n",
        "    action = agent.step(reward = r, discount = 0.9, next_state = ns)\n",
        "    observation = case.play(action)\n",
        "    mean_reward += (r - mean_reward)/(i + 1.)\n",
        "\n",
        "  reward_hist_each_episode_DynaQ.append(mean_reward)\n",
        "  for __ in range(19):\n",
        "    agent = DynaQ(4, 99, 24, epsilon_greedy, Q, num_offline_updates=100, step_size=0.2)\n",
        "    case =  grid_world.make_game()\n",
        "    observation = case.its_showtime()\n",
        "    observation = case.play(0)\n",
        "    mean_reward = 0.\n",
        "    for i in range(100):\n",
        "      r = observation[1]\n",
        "      if observation[1] == None:\n",
        "        r = 0\n",
        "      ns = basic.location(observation, char='O')\n",
        "      action = agent.step(reward = r, discount = 0.9, next_state = ns)\n",
        "      observation = case.play(action)\n",
        "      mean_reward += (r - mean_reward)/(i + 1.)\n",
        "    reward_hist_each_episode_DynaQ.append(mean_reward)\n",
        "  learned_Q.append(Q)\n",
        "  reward_hist_each_experiment_DynaQ.append(reward_hist_each_episode_DynaQ)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvAwM96iomiP"
      },
      "source": [
        "# learned action-value function\n",
        "np.mean(np.array(learned_Q),axis =0)[24,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[13,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[14,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[15,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[16,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[17,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[18,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[19,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[30,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[31,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[41,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[29,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[28,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[23,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[25,:]\n",
        "np.mean(np.array(learned_Q),axis =0)[35,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGmOzO6zllPm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55W9NXhgpNGo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ViRZ7aypGKx"
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "red_patch = mpatches.Patch(color='red', label='EM-ML')\n",
        "black_patch = mpatches.Patch(color='black', label='EM-VB')\n",
        "blue_patch = mpatches.Patch(color='blue', label='Dyna-Q')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DCMtGLspGOP"
      },
      "source": [
        "emml_reward_data = np.array(reward_hist_each_experiment_EM_ML)\n",
        "emvb_reward_data = np.array(reward_hist_each_experiment_EM_VB)\n",
        "DynaQ_reward_data = np.array(reward_hist_each_experiment_DynaQ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9roPfJDhllR6"
      },
      "source": [
        "log = pd.DataFrame(columns=['episodes','average reward','Algorithm'])\n",
        "for nx in range(50):\n",
        "    for ne in range(20):\n",
        "        log=log.append(pd.DataFrame({'episodes': ne+1,\n",
        "                                    'average reward': emml_reward_data[nx, ne],\n",
        "                                    'Algorithm': \"EM-ML\"}, index=[log.size+1]))\n",
        "log1 = pd.DataFrame(columns=['episodes','average reward','Algorithm'])\n",
        "for nx in range(50):\n",
        "    for ne in range(20):\n",
        "        log1=log1.append(pd.DataFrame({'episodes': ne+1,\n",
        "                                    'average reward': emvb_reward_data[nx, ne],\n",
        "                                    'Algorithm': \"EM-VB\"}, index=[log1.size+1]))\n",
        "log2 = pd.DataFrame(columns=['episodes','average reward','Algorithm'])\n",
        "for nx in range(50):\n",
        "    for ne in range(20):\n",
        "        log2=log2.append(pd.DataFrame({'episodes': ne+1,\n",
        "                                    'average reward': DynaQ_reward_data[nx, ne],\n",
        "                                    'Algorithm': \"Dyna-Q\"}, index=[log2.size+1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbh2pbG5pTMH"
      },
      "source": [
        "sns.set_style('white')\n",
        "A = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log, color=\"red\", label = \"EM-ML\", ci = 90)\n",
        "B = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log1, color=\"black\", label = \"EM-VB\", ci = 90)\n",
        "C = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log2, color= \"blue\", label = \"Dyna-Q\", ci = 90)\n",
        "plt.legend(handles=[red_patch, black_patch, blue_patch])\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('average reward')\n",
        "plt.title('EM-ML, EM-VB and Dyna-Q on chain problem with confidence interval')\n",
        "plt.savefig('ENV2_with_ci')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYX9CuKzpVrX"
      },
      "source": [
        "A = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log, color=\"red\", label = \"EM-ML\", ci = None)\n",
        "B = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log1, color=\"black\", label = \"EM-VB\", ci= None)\n",
        "C = sns.pointplot(x=\"episodes\", y=\"average reward\", data=log2, color= \"blue\", label = \"Dyna-Q\", ci = None)\n",
        "plt.legend(handles=[red_patch, black_patch, blue_patch])\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('average reward')\n",
        "plt.title('EM-ML, EM-VB and Dyna-Q on chain problem')\n",
        "plt.savefig('ENV2_without_ci')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}